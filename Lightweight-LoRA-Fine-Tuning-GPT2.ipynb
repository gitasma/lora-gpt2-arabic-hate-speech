{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c42051d",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "\n",
    "This notebook demonstrates a lightweight fine-tuning pipeline using the Parameter-Efficient Fine-Tuning (PEFT) approach with LoRA adapters. My choices for this project are:\n",
    "\n",
    "- **Model:** `gpt2` as the base sequence classification model, selected for its balance of size and capability.\n",
    "- **Evaluation approach:** We evaluate both the base and fine-tuned models on the test split using the Hugging Face `Trainer` with accuracy as our primary metric.\n",
    "- **Fine-tuning dataset:** The **Egyptian Arabic Hate Speech** dataset (`IbrahimAmin/egyptian-arabic-hate-speech`), which contains multi-class labels ('Religious Discrimination', 'Offensive', 'Racism', 'Neutral', 'Sexism)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "In this section, we load and preprocess the foundation model and dataset:\n",
    "\n",
    "1. **Dataset Loading:** We fetch the `IbrahimAmin/egyptian-arabic-hate-speech` dataset and create label-to-id mappings.\n",
    "2. **Tokenization:** We use `AutoTokenizer` from the `transformers` library to tokenize the text inputs, ensuring padding and truncation.\n",
    "3. **Baseline Evaluation:** We instantiate the pre-trained GPT-2 sequence classification model without any fine-tuning and evaluate it on the test set to establish a performance baseline (accuracy).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from peft import AutoPeftModelForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "from transformers import BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "702ffb66c02e49ba8bccc5443e2cd5de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 420k/420k [00:00<00:00, 2.11MB/s]\n",
      "Downloading data: 100%|██████████| 108k/108k [00:00<00:00, 1.43MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25a5f05c4f774f5899460d99768c69ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/6535 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36112dca58d14e7eb53f59dfc14009a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1634 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 6535\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 1634\n",
      "    })\n",
      "})\n",
      "{0: 'Religious Discrimination', 1: 'Offensive', 2: 'Racism', 3: 'Neutral', 4: 'Sexism'}\n",
      "{'Religious Discrimination': 0, 'Offensive': 1, 'Racism': 2, 'Neutral': 3, 'Sexism': 4}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"IbrahimAmin/egyptian-arabic-hate-speech\")\n",
    "for split in ['train', 'test']:\n",
    "    dataset[split].shuffle(seed=42)\n",
    "print(dataset)\n",
    "id2label   = { i: name for i, name in enumerate(set(dataset['test']['label'])) }\n",
    "label2id   = { name: i for i, name in enumerate(set(dataset['test']['label'])) }\n",
    "print(id2label)\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe017682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels information\n",
      "Number of classes - train: 5\n",
      "Number of classes - test : 5\n",
      "Labels - train: {'Religious Discrimination', 'Offensive', 'Racism', 'Neutral', 'Sexism'}\n",
      "Labels - test : {'Religious Discrimination', 'Offensive', 'Racism', 'Neutral', 'Sexism'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Labels information\")\n",
    "num_labels = len(set(dataset['train']['label']))\n",
    "print(f\"Number of classes - train: {num_labels}\")\n",
    "num_labels = len(set(dataset['test']['label']))\n",
    "print(f\"Number of classes - test : {num_labels}\")\n",
    "print(f\"Labels - train: {set(dataset['train']['label'])}\")\n",
    "print(f\"Labels - test : {set(dataset['test']['label'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b25afe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "284f7f63de6f4c1580afc0ec09daf25a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3571fc774d8a4720b4244a0b84b0a5ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9f92985ee947b980934184e77cbc72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eebdab1f7b9407eb5b9ddd8b793d8ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88bbe98019c846bb90e6cfdfcf302c02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecd72eeb7539414686e76d0d5473a12a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "base_model.config.pad_token_id = tokenizer.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bde3cb2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2ForSequenceClassification(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=5, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfbfe750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight: Parameter containing:\n",
      "tensor([[ 0.0031,  0.0055,  0.0034,  ...,  0.0051, -0.0155, -0.0438],\n",
      "        [ 0.0113,  0.0241, -0.0040,  ...,  0.0078, -0.0217, -0.0076],\n",
      "        [-0.0054, -0.0023, -0.0147,  ...,  0.0127,  0.0315, -0.0270],\n",
      "        [-0.0137,  0.0243, -0.0260,  ..., -0.0130,  0.0027,  0.0454],\n",
      "        [ 0.0168,  0.0175, -0.0219,  ..., -0.0233, -0.0272,  0.0018]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "head = base_model.score\n",
    "for n, p in head.named_parameters():\n",
    "    print(f\"{n}: {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fa09830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4bb774b2e474183b544c3fe71aafd2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6535 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64bcd60cef984463bf7aec2d83d371b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1634 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b058682b70f40d586fa74b748dc1dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6535 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef181f3aa6b64e7280228af097ab4f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1634 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def add_label_id(example):\n",
    "    example[\"label_id\"] = label2id[example[\"label\"]]\n",
    "    return example\n",
    "\n",
    "def tokenize_and_attach_id(example):\n",
    "    tokenized = tokenizer(\n",
    "        example[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    tokenized[\"labels\"] = example[\"label_id\"]\n",
    "    return tokenized\n",
    "\n",
    "dataset = dataset.map(add_label_id)\n",
    "tokenized_dataset = {}\n",
    "for split in ['train', 'test']:\n",
    "    tokenized_dataset[split] = dataset[split].map(tokenize_and_attach_id,\n",
    "                                                  batched=True,\n",
    "                                                  remove_columns=dataset[split].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cefdd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "البنات مش نافعين لا في كوره ولا في السياسه ولا في شغل ولا في أي حاجه خالص\n",
      "\n",
      "[23525, 39848, 23338, 34247, 103, 47048, 148, 112, 18923, 228, 12919, 149, 223, 44690, 22654, 23338, 220, 13862, 12919, 18923, 223, 22654, 18923, 225, 30335, 26897, 29519, 42092, 13862, 12919, 18923, 223, 22654, 28981, 45692, 22654, 34247, 111, 29519, 42092, 13862, 12919, 18923, 223, 22654, 17550, 112, 148, 118, 13862, 42092, 13862, 12919, 18923, 223, 22654, 17550, 96, 22654, 17550, 255, 34247, 105, 29519, 17550, 106, 23525, 148, 113, 50256]\n",
      "\n",
      "['Ø§ÙĦ', 'Ø¨', 'ÙĨ', 'Ø§Ø', 'ª', 'ĠÙħ', 'Ø', '´', 'ĠÙ', 'Ĩ', 'Ø§', 'Ù', 'ģ', 'Ø¹', 'ÙĬ', 'ÙĨ', 'Ġ', 'ÙĦ', 'Ø§', 'ĠÙ', 'ģ', 'ÙĬ', 'ĠÙ', 'ĥ', 'ÙĪ', 'Ø±', 'Ùĩ', 'ĠÙĪ', 'ÙĦ', 'Ø§', 'ĠÙ', 'ģ', 'ÙĬ', 'ĠØ§ÙĦ', 'Ø³', 'ÙĬ', 'Ø§Ø', '³', 'Ùĩ', 'ĠÙĪ', 'ÙĦ', 'Ø§', 'ĠÙ', 'ģ', 'ÙĬ', 'ĠØ', '´', 'Ø', 'º', 'ÙĦ', 'ĠÙĪ', 'ÙĦ', 'Ø§', 'ĠÙ', 'ģ', 'ÙĬ', 'ĠØ', '£', 'ÙĬ', 'ĠØ', 'Ń', 'Ø§Ø', '¬', 'Ùĩ', 'ĠØ', '®', 'Ø§ÙĦ', 'Ø', 'µ', '<|endoftext|>']\n",
      "\n",
      "البنات مش نافعين لا في كوره ولا في السياسه ولا في شغل ولا في أي حاجه خالص\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[\"train\"][3445]\n",
    "print(f\"{dataset['train'][3445]['text']}\\n\")\n",
    "tokenized_sample = tokenized_dataset[\"train\"][3445][\"input_ids\"]\n",
    "print(f\"{tokenized_sample[0:70]}\\n\")\n",
    "if hasattr(tokenized_sample, \"tolist\"):\n",
    "    tokenized_sample = tokenized_sample.tolist()\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_sample)\n",
    "print(f\"{tokens[0:70]}\\n\")\n",
    "reconstructed = tokenizer.convert_tokens_to_string(tokens)[0:73]\n",
    "print(reconstructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fc78d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.2939870357513428, 'eval_accuracy': 0.1835985312117503, 'eval_runtime': 131.1589, 'eval_samples_per_second': 12.458, 'eval_steps_per_second': 0.785}\n"
     ]
    }
   ],
   "source": [
    "base_model_trainer = Trainer(\n",
    "    model= base_model,\n",
    "    args= TrainingArguments(\n",
    "    output_dir= \"./base_model_evaluation\",\n",
    "    per_device_eval_batch_size= 16,\n",
    "    do_train= False,\n",
    "    do_eval= True,\n",
    "),\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "base_model_metrics = base_model_trainer.evaluate()\n",
    "print(base_model_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a68d528b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>الخليجيين مبيستحموش واكتر ناس معفنا ونتنا ممكن...</td>\n",
       "      <td>Racism</td>\n",
       "      <td>Racism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>طريق زحمه قوي مش عارف هيفضل زحمه كده لحد امتي</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Racism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ده حتي فيه زنوج بيترياو علي بعض من كتر ما هما ...</td>\n",
       "      <td>Racism</td>\n",
       "      <td>Racism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>انا بزعل امي كتير ربنا يسامحني</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Racism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>انا نفسي في موبايل جديد بس مش معايا فلوس</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Racism</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    label predicted_label\n",
       "0  الخليجيين مبيستحموش واكتر ناس معفنا ونتنا ممكن...   Racism          Racism\n",
       "1      طريق زحمه قوي مش عارف هيفضل زحمه كده لحد امتي  Neutral          Racism\n",
       "2  ده حتي فيه زنوج بيترياو علي بعض من كتر ما هما ...   Racism          Racism\n",
       "3                     انا بزعل امي كتير ربنا يسامحني  Neutral          Racism\n",
       "4           انا نفسي في موبايل جديد بس مش معايا فلوس  Neutral          Racism"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_test = dataset[\"test\"]\n",
    "pred_out = base_model_trainer.predict(tokenized_dataset[\"test\"])\n",
    "pred_ids = np.argmax(pred_out.predictions, axis=1)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"text\": raw_test[\"text\"],\n",
    "    \"label\": raw_test[\"label\"],\n",
    "    \"predicted_label\": [ id2label[i] for i in pred_ids ],\n",
    "})\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "Here, we apply LoRA to the base model for parameter-efficient training:\n",
    "\n",
    "1. **LoRA Configuration:** Define the rank, alpha, dropout, and target modules for adapter insertion.\n",
    "2. **PEFT Model Setup:** Wrap the base GPT-2 model with LoRA adapters, enabling low-rank updates.\n",
    "3. **Training Loop:** Use the Hugging Face `Trainer` to fine-tune only the adapter weights on the training split, significantly reducing the number of trainable parameters.\n",
    "4. **Saving Weights:** Persist the resulting PEFT model weights for later inference and comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/tuners/lora.py:475: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"c_attn\"],\n",
    "    lora_dropout=0.02,\n",
    "    task_type=\"SEQ_CLS\",\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(base_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9053477e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 597,504 || all params: 125,037,312 || trainable%: 0.4778605605341228\n"
     ]
    }
   ],
   "source": [
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be6ee105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2ForSequenceClassification(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(50257, 768)\n",
       "        (wpe): Embedding(1024, 768)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-11): 12 x GPT2Block(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2Attention(\n",
       "              (c_attn): Linear(\n",
       "                in_features=768, out_features=2304, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.02, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (c_proj): Conv1D()\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=5, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=5, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08a05d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_module.weight: Parameter containing:\n",
      "tensor([[ 0.0031,  0.0055,  0.0034,  ...,  0.0051, -0.0155, -0.0438],\n",
      "        [ 0.0113,  0.0241, -0.0040,  ...,  0.0078, -0.0217, -0.0076],\n",
      "        [-0.0054, -0.0023, -0.0147,  ...,  0.0127,  0.0315, -0.0270],\n",
      "        [-0.0137,  0.0243, -0.0260,  ..., -0.0130,  0.0027,  0.0454],\n",
      "        [ 0.0168,  0.0175, -0.0219,  ..., -0.0233, -0.0272,  0.0018]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "modules_to_save.default.weight: Parameter containing:\n",
      "tensor([[ 0.0031,  0.0055,  0.0034,  ...,  0.0051, -0.0155, -0.0438],\n",
      "        [ 0.0113,  0.0241, -0.0040,  ...,  0.0078, -0.0217, -0.0076],\n",
      "        [-0.0054, -0.0023, -0.0147,  ...,  0.0127,  0.0315, -0.0270],\n",
      "        [-0.0137,  0.0243, -0.0260,  ..., -0.0130,  0.0027,  0.0454],\n",
      "        [ 0.0168,  0.0175, -0.0219,  ..., -0.0233, -0.0272,  0.0018]],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "head = lora_model.model.score\n",
    "for n, p in head.named_parameters():\n",
    "    print(f\"{n}: {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "894046c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OptimizerNames.ADAMW_TORCH\n"
     ]
    }
   ],
   "source": [
    "lora_trainer = Trainer(\n",
    "    model= lora_model,\n",
    "    args= TrainingArguments(\n",
    "    output_dir= \"lora-gpt2\",\n",
    "    per_device_train_batch_size= 2,\n",
    "    per_device_eval_batch_size= 2,\n",
    "    gradient_accumulation_steps= 2,\n",
    "    evaluation_strategy= \"epoch\",\n",
    "    save_strategy= \"epoch\",\n",
    "    num_train_epochs= 6,\n",
    "    learning_rate=5e-4,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end= True,\n",
    "),\n",
    "    train_dataset= tokenized_dataset[\"train\"],\n",
    "    eval_dataset= tokenized_dataset[\"test\"],\n",
    "    tokenizer= tokenizer,\n",
    "    data_collator= DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics= compute_metrics,\n",
    ")\n",
    "\n",
    "print(lora_trainer.args.optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0316cc60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9804' max='9804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9804/9804 2:33:27, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.310300</td>\n",
       "      <td>1.050025</td>\n",
       "      <td>0.591799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.933500</td>\n",
       "      <td>0.930773</td>\n",
       "      <td>0.671359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.775300</td>\n",
       "      <td>0.737174</td>\n",
       "      <td>0.753366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.659200</td>\n",
       "      <td>0.717998</td>\n",
       "      <td>0.771726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.596100</td>\n",
       "      <td>0.739166</td>\n",
       "      <td>0.793758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>0.733569</td>\n",
       "      <td>0.805386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9804, training_loss=0.8413012927524998, metrics={'train_runtime': 9208.0135, 'train_samples_per_second': 4.258, 'train_steps_per_second': 1.065, 'total_flos': 2.063446359146496e+16, 'train_loss': 0.8413012927524998, 'epoch': 6.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e8a663",
   "metadata": {},
   "source": [
    "###  ⚠️ IMPORTANT ⚠️\n",
    "\n",
    "Due to workspace storage constraints, you should not store the model weights in the same directory but rather use `/tmp` to avoid workspace crashes which are irrecoverable.\n",
    "Ensure you save it in /tmp always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "lora_model.save_pretrained(\"temp/lora-gpt2/best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "In this final section, we load the saved PEFT model and run inference on sample inputs from the test set:\n",
    "\n",
    "1. **Model Loading:** Load the fine-tuned LoRA adapter weights into the base GPT-2 model.\n",
    "2. **Prediction:** Generate predictions on a subset of test examples and compute accuracy.\n",
    "3. **Comparison:** Compare the inference results of the fine-tuned model against the baseline to quantify improvements in classification accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2ForSequenceClassification(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(50257, 768)\n",
       "        (wpe): Embedding(1024, 768)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-11): 12 x GPT2Block(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2Attention(\n",
       "              (c_attn): Linear(\n",
       "                in_features=768, out_features=2304, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.02, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (c_proj): Conv1D()\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=5, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=5, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_model = AutoPeftModelForSequenceClassification.from_pretrained(\n",
    "    \"temp/lora-gpt2/best\",\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    is_trainable=False\n",
    ")\n",
    "finetuned_model.config.pad_token_id = tokenizer.eos_token_id\n",
    "finetuned_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7179978489875793, 'eval_accuracy': 0.7717258261933905, 'eval_runtime': 139.6509, 'eval_samples_per_second': 11.701, 'eval_steps_per_second': 0.738}\n"
     ]
    }
   ],
   "source": [
    "finetuned_model_trainer = Trainer(\n",
    "    model=finetuned_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./finetuned_model_evaluation\",\n",
    "        per_device_eval_batch_size=16,\n",
    "        do_train=False,\n",
    "        do_eval=True,\n",
    "    ),\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "finetuned_model_metrics = finetuned_model_trainer.evaluate()\n",
    "print(finetuned_model_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d036b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>الخليجيين مبيستحموش واكتر ناس معفنا ونتنا ممكن...</td>\n",
       "      <td>Racism</td>\n",
       "      <td>Racism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>طريق زحمه قوي مش عارف هيفضل زحمه كده لحد امتي</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ده حتي فيه زنوج بيترياو علي بعض من كتر ما هما ...</td>\n",
       "      <td>Racism</td>\n",
       "      <td>Racism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>انا بزعل امي كتير ربنا يسامحني</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>انا نفسي في موبايل جديد بس مش معايا فلوس</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    label predicted_label\n",
       "0  الخليجيين مبيستحموش واكتر ناس معفنا ونتنا ممكن...   Racism          Racism\n",
       "1      طريق زحمه قوي مش عارف هيفضل زحمه كده لحد امتي  Neutral         Neutral\n",
       "2  ده حتي فيه زنوج بيترياو علي بعض من كتر ما هما ...   Racism          Racism\n",
       "3                     انا بزعل امي كتير ربنا يسامحني  Neutral         Neutral\n",
       "4           انا نفسي في موبايل جديد بس مش معايا فلوس  Neutral         Neutral"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_pred_out = finetuned_model_trainer.predict(tokenized_dataset[\"test\"])\n",
    "finetuned_pred_ids = np.argmax(finetuned_pred_out.predictions, axis=1)\n",
    "\n",
    "df_finetuned = pd.DataFrame({\n",
    "    \"text\": raw_test[\"text\"],\n",
    "    \"label\": raw_test[\"label\"],\n",
    "    \"predicted_label\": [ id2label[i] for i in finetuned_pred_ids ],\n",
    "})\n",
    "\n",
    "df_finetuned.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained gpt2 model accuracy:     0.1835985312117503\n",
      "Finetuned LoRA-gpt2 model accuracy: 0.7717258261933905\n"
     ]
    }
   ],
   "source": [
    "print(\"Pretrained gpt2 model accuracy:    \", base_model_metrics[\"eval_accuracy\"])\n",
    "print(\"Finetuned LoRA-gpt2 model accuracy:\", finetuned_model_metrics[\"eval_accuracy\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
